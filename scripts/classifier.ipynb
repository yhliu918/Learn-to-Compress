{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import struct\n",
    "from scipy.stats import norm, lognorm\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import pdb\n",
    "import joblib\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd=np.loadtxt('train.txt',skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dd[:,:-1]\n",
    "y = dd[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\n",
    "                                                     random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:35:07] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:35:07] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softmax' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\ttrain-mlogloss:1.10356\ttest-mlogloss:1.10379\n",
      "[1]\ttrain-mlogloss:0.91713\ttest-mlogloss:0.91763\n",
      "[2]\ttrain-mlogloss:0.78289\ttest-mlogloss:0.78384\n",
      "[3]\ttrain-mlogloss:0.68220\ttest-mlogloss:0.68294\n",
      "[4]\ttrain-mlogloss:0.60428\ttest-mlogloss:0.60526\n",
      "[5]\ttrain-mlogloss:0.54320\ttest-mlogloss:0.54434\n",
      "[6]\ttrain-mlogloss:0.49493\ttest-mlogloss:0.49591\n",
      "[7]\ttrain-mlogloss:0.45599\ttest-mlogloss:0.45720\n",
      "[8]\ttrain-mlogloss:0.42464\ttest-mlogloss:0.42589\n",
      "[9]\ttrain-mlogloss:0.39896\ttest-mlogloss:0.40036\n",
      "[10]\ttrain-mlogloss:0.37805\ttest-mlogloss:0.37945\n",
      "[11]\ttrain-mlogloss:0.36069\ttest-mlogloss:0.36227\n",
      "[12]\ttrain-mlogloss:0.34648\ttest-mlogloss:0.34799\n",
      "[13]\ttrain-mlogloss:0.33455\ttest-mlogloss:0.33616\n",
      "[14]\ttrain-mlogloss:0.32460\ttest-mlogloss:0.32624\n",
      "[15]\ttrain-mlogloss:0.31627\ttest-mlogloss:0.31792\n",
      "[16]\ttrain-mlogloss:0.30927\ttest-mlogloss:0.31095\n",
      "[17]\ttrain-mlogloss:0.30330\ttest-mlogloss:0.30501\n",
      "[18]\ttrain-mlogloss:0.29828\ttest-mlogloss:0.29996\n",
      "[19]\ttrain-mlogloss:0.29397\ttest-mlogloss:0.29570\n",
      "[20]\ttrain-mlogloss:0.29031\ttest-mlogloss:0.29204\n",
      "[21]\ttrain-mlogloss:0.28716\ttest-mlogloss:0.28895\n",
      "[22]\ttrain-mlogloss:0.28448\ttest-mlogloss:0.28625\n",
      "[23]\ttrain-mlogloss:0.28210\ttest-mlogloss:0.28396\n",
      "[24]\ttrain-mlogloss:0.28013\ttest-mlogloss:0.28196\n",
      "[25]\ttrain-mlogloss:0.27829\ttest-mlogloss:0.28022\n",
      "[26]\ttrain-mlogloss:0.27679\ttest-mlogloss:0.27875\n",
      "[27]\ttrain-mlogloss:0.27550\ttest-mlogloss:0.27741\n",
      "[28]\ttrain-mlogloss:0.27431\ttest-mlogloss:0.27629\n",
      "[29]\ttrain-mlogloss:0.27323\ttest-mlogloss:0.27528\n",
      "[30]\ttrain-mlogloss:0.27235\ttest-mlogloss:0.27441\n",
      "[31]\ttrain-mlogloss:0.27154\ttest-mlogloss:0.27361\n",
      "[32]\ttrain-mlogloss:0.27081\ttest-mlogloss:0.27294\n",
      "[33]\ttrain-mlogloss:0.27011\ttest-mlogloss:0.27234\n",
      "[34]\ttrain-mlogloss:0.26960\ttest-mlogloss:0.27184\n",
      "[35]\ttrain-mlogloss:0.26904\ttest-mlogloss:0.27138\n",
      "[36]\ttrain-mlogloss:0.26862\ttest-mlogloss:0.27096\n",
      "[37]\ttrain-mlogloss:0.26814\ttest-mlogloss:0.27062\n",
      "[38]\ttrain-mlogloss:0.26775\ttest-mlogloss:0.27030\n",
      "[39]\ttrain-mlogloss:0.26744\ttest-mlogloss:0.27001\n",
      "[40]\ttrain-mlogloss:0.26716\ttest-mlogloss:0.26975\n",
      "[41]\ttrain-mlogloss:0.26679\ttest-mlogloss:0.26954\n",
      "[42]\ttrain-mlogloss:0.26653\ttest-mlogloss:0.26935\n",
      "[43]\ttrain-mlogloss:0.26628\ttest-mlogloss:0.26916\n",
      "[44]\ttrain-mlogloss:0.26608\ttest-mlogloss:0.26902\n",
      "[45]\ttrain-mlogloss:0.26586\ttest-mlogloss:0.26882\n",
      "[46]\ttrain-mlogloss:0.26570\ttest-mlogloss:0.26870\n",
      "[47]\ttrain-mlogloss:0.26552\ttest-mlogloss:0.26860\n",
      "[48]\ttrain-mlogloss:0.26536\ttest-mlogloss:0.26849\n",
      "[49]\ttrain-mlogloss:0.26524\ttest-mlogloss:0.26840\n",
      "[50]\ttrain-mlogloss:0.26509\ttest-mlogloss:0.26832\n",
      "[51]\ttrain-mlogloss:0.26497\ttest-mlogloss:0.26823\n",
      "[52]\ttrain-mlogloss:0.26476\ttest-mlogloss:0.26815\n",
      "[53]\ttrain-mlogloss:0.26467\ttest-mlogloss:0.26808\n",
      "[54]\ttrain-mlogloss:0.26454\ttest-mlogloss:0.26803\n",
      "[55]\ttrain-mlogloss:0.26446\ttest-mlogloss:0.26798\n",
      "[56]\ttrain-mlogloss:0.26429\ttest-mlogloss:0.26793\n",
      "[57]\ttrain-mlogloss:0.26413\ttest-mlogloss:0.26791\n",
      "[58]\ttrain-mlogloss:0.26408\ttest-mlogloss:0.26787\n",
      "[59]\ttrain-mlogloss:0.26396\ttest-mlogloss:0.26784\n",
      "[60]\ttrain-mlogloss:0.26384\ttest-mlogloss:0.26782\n",
      "[61]\ttrain-mlogloss:0.26372\ttest-mlogloss:0.26781\n",
      "[62]\ttrain-mlogloss:0.26362\ttest-mlogloss:0.26780\n",
      "[63]\ttrain-mlogloss:0.26358\ttest-mlogloss:0.26776\n",
      "[64]\ttrain-mlogloss:0.26350\ttest-mlogloss:0.26774\n",
      "[65]\ttrain-mlogloss:0.26347\ttest-mlogloss:0.26773\n",
      "[66]\ttrain-mlogloss:0.26336\ttest-mlogloss:0.26773\n",
      "[67]\ttrain-mlogloss:0.26318\ttest-mlogloss:0.26776\n",
      "[68]\ttrain-mlogloss:0.26311\ttest-mlogloss:0.26775\n",
      "[69]\ttrain-mlogloss:0.26303\ttest-mlogloss:0.26775\n",
      "[70]\ttrain-mlogloss:0.26290\ttest-mlogloss:0.26775\n",
      "[71]\ttrain-mlogloss:0.26288\ttest-mlogloss:0.26775\n",
      "[72]\ttrain-mlogloss:0.26279\ttest-mlogloss:0.26775\n",
      "[73]\ttrain-mlogloss:0.26270\ttest-mlogloss:0.26774\n",
      "[74]\ttrain-mlogloss:0.26261\ttest-mlogloss:0.26774\n",
      "[75]\ttrain-mlogloss:0.26249\ttest-mlogloss:0.26776\n",
      "[76]\ttrain-mlogloss:0.26240\ttest-mlogloss:0.26777\n",
      "[77]\ttrain-mlogloss:0.26233\ttest-mlogloss:0.26777\n",
      "[78]\ttrain-mlogloss:0.26227\ttest-mlogloss:0.26778\n",
      "[79]\ttrain-mlogloss:0.26221\ttest-mlogloss:0.26778\n",
      "[80]\ttrain-mlogloss:0.26216\ttest-mlogloss:0.26780\n",
      "[81]\ttrain-mlogloss:0.26213\ttest-mlogloss:0.26779\n",
      "[82]\ttrain-mlogloss:0.26211\ttest-mlogloss:0.26779\n",
      "[83]\ttrain-mlogloss:0.26206\ttest-mlogloss:0.26780\n",
      "[84]\ttrain-mlogloss:0.26203\ttest-mlogloss:0.26781\n",
      "[85]\ttrain-mlogloss:0.26194\ttest-mlogloss:0.26782\n",
      "[86]\ttrain-mlogloss:0.26189\ttest-mlogloss:0.26783\n",
      "[87]\ttrain-mlogloss:0.26176\ttest-mlogloss:0.26786\n",
      "[88]\ttrain-mlogloss:0.26167\ttest-mlogloss:0.26790\n",
      "[89]\ttrain-mlogloss:0.26164\ttest-mlogloss:0.26791\n",
      "[90]\ttrain-mlogloss:0.26154\ttest-mlogloss:0.26793\n",
      "[91]\ttrain-mlogloss:0.26151\ttest-mlogloss:0.26793\n",
      "[92]\ttrain-mlogloss:0.26149\ttest-mlogloss:0.26793\n",
      "[93]\ttrain-mlogloss:0.26141\ttest-mlogloss:0.26793\n",
      "[94]\ttrain-mlogloss:0.26138\ttest-mlogloss:0.26794\n",
      "[95]\ttrain-mlogloss:0.26138\ttest-mlogloss:0.26794\n",
      "[96]\ttrain-mlogloss:0.26128\ttest-mlogloss:0.26794\n",
      "[97]\ttrain-mlogloss:0.26120\ttest-mlogloss:0.26797\n",
      "[98]\ttrain-mlogloss:0.26110\ttest-mlogloss:0.26798\n",
      "[99]\ttrain-mlogloss:0.26104\ttest-mlogloss:0.26798\n",
      "[100]\ttrain-mlogloss:0.26096\ttest-mlogloss:0.26798\n",
      "[101]\ttrain-mlogloss:0.26091\ttest-mlogloss:0.26799\n",
      "[102]\ttrain-mlogloss:0.26084\ttest-mlogloss:0.26798\n",
      "[103]\ttrain-mlogloss:0.26079\ttest-mlogloss:0.26800\n",
      "[104]\ttrain-mlogloss:0.26073\ttest-mlogloss:0.26801\n",
      "[105]\ttrain-mlogloss:0.26069\ttest-mlogloss:0.26800\n",
      "[106]\ttrain-mlogloss:0.26063\ttest-mlogloss:0.26800\n",
      "[107]\ttrain-mlogloss:0.26059\ttest-mlogloss:0.26800\n",
      "[108]\ttrain-mlogloss:0.26059\ttest-mlogloss:0.26800\n",
      "[109]\ttrain-mlogloss:0.26057\ttest-mlogloss:0.26800\n",
      "[110]\ttrain-mlogloss:0.26050\ttest-mlogloss:0.26803\n",
      "[111]\ttrain-mlogloss:0.26046\ttest-mlogloss:0.26805\n",
      "[112]\ttrain-mlogloss:0.26043\ttest-mlogloss:0.26805\n",
      "[113]\ttrain-mlogloss:0.26038\ttest-mlogloss:0.26805\n",
      "[114]\ttrain-mlogloss:0.26038\ttest-mlogloss:0.26805\n",
      "[115]\ttrain-mlogloss:0.26030\ttest-mlogloss:0.26807\n",
      "[116]\ttrain-mlogloss:0.26029\ttest-mlogloss:0.26807\n",
      "[117]\ttrain-mlogloss:0.26026\ttest-mlogloss:0.26807\n",
      "[118]\ttrain-mlogloss:0.26020\ttest-mlogloss:0.26806\n",
      "[119]\ttrain-mlogloss:0.26017\ttest-mlogloss:0.26808\n",
      "[120]\ttrain-mlogloss:0.26011\ttest-mlogloss:0.26810\n",
      "[121]\ttrain-mlogloss:0.26004\ttest-mlogloss:0.26811\n",
      "[122]\ttrain-mlogloss:0.25999\ttest-mlogloss:0.26810\n",
      "[123]\ttrain-mlogloss:0.25997\ttest-mlogloss:0.26810\n",
      "[124]\ttrain-mlogloss:0.25993\ttest-mlogloss:0.26810\n",
      "[125]\ttrain-mlogloss:0.25985\ttest-mlogloss:0.26813\n",
      "[126]\ttrain-mlogloss:0.25978\ttest-mlogloss:0.26815\n",
      "[127]\ttrain-mlogloss:0.25973\ttest-mlogloss:0.26817\n",
      "[128]\ttrain-mlogloss:0.25969\ttest-mlogloss:0.26816\n",
      "[129]\ttrain-mlogloss:0.25959\ttest-mlogloss:0.26819\n",
      "[130]\ttrain-mlogloss:0.25945\ttest-mlogloss:0.26821\n",
      "[131]\ttrain-mlogloss:0.25937\ttest-mlogloss:0.26821\n",
      "[132]\ttrain-mlogloss:0.25936\ttest-mlogloss:0.26822\n",
      "[133]\ttrain-mlogloss:0.25930\ttest-mlogloss:0.26821\n",
      "[134]\ttrain-mlogloss:0.25923\ttest-mlogloss:0.26823\n",
      "[135]\ttrain-mlogloss:0.25919\ttest-mlogloss:0.26823\n",
      "[136]\ttrain-mlogloss:0.25911\ttest-mlogloss:0.26826\n",
      "[137]\ttrain-mlogloss:0.25900\ttest-mlogloss:0.26827\n",
      "[138]\ttrain-mlogloss:0.25894\ttest-mlogloss:0.26828\n",
      "[139]\ttrain-mlogloss:0.25890\ttest-mlogloss:0.26829\n",
      "[140]\ttrain-mlogloss:0.25889\ttest-mlogloss:0.26829\n",
      "[141]\ttrain-mlogloss:0.25889\ttest-mlogloss:0.26829\n",
      "[142]\ttrain-mlogloss:0.25888\ttest-mlogloss:0.26830\n",
      "[143]\ttrain-mlogloss:0.25881\ttest-mlogloss:0.26833\n",
      "[144]\ttrain-mlogloss:0.25881\ttest-mlogloss:0.26833\n",
      "[145]\ttrain-mlogloss:0.25881\ttest-mlogloss:0.26833\n",
      "[146]\ttrain-mlogloss:0.25876\ttest-mlogloss:0.26833\n",
      "[147]\ttrain-mlogloss:0.25874\ttest-mlogloss:0.26834\n",
      "[148]\ttrain-mlogloss:0.25870\ttest-mlogloss:0.26834\n",
      "[149]\ttrain-mlogloss:0.25867\ttest-mlogloss:0.26834\n",
      "[150]\ttrain-mlogloss:0.25863\ttest-mlogloss:0.26836\n",
      "[151]\ttrain-mlogloss:0.25861\ttest-mlogloss:0.26836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[152]\ttrain-mlogloss:0.25857\ttest-mlogloss:0.26837\n",
      "[153]\ttrain-mlogloss:0.25851\ttest-mlogloss:0.26837\n",
      "[154]\ttrain-mlogloss:0.25840\ttest-mlogloss:0.26839\n",
      "[155]\ttrain-mlogloss:0.25833\ttest-mlogloss:0.26839\n",
      "[156]\ttrain-mlogloss:0.25825\ttest-mlogloss:0.26837\n",
      "[157]\ttrain-mlogloss:0.25818\ttest-mlogloss:0.26840\n",
      "[158]\ttrain-mlogloss:0.25811\ttest-mlogloss:0.26842\n",
      "[159]\ttrain-mlogloss:0.25810\ttest-mlogloss:0.26841\n",
      "[160]\ttrain-mlogloss:0.25807\ttest-mlogloss:0.26842\n",
      "[161]\ttrain-mlogloss:0.25804\ttest-mlogloss:0.26842\n",
      "[162]\ttrain-mlogloss:0.25800\ttest-mlogloss:0.26843\n",
      "[163]\ttrain-mlogloss:0.25796\ttest-mlogloss:0.26844\n",
      "[164]\ttrain-mlogloss:0.25792\ttest-mlogloss:0.26845\n",
      "[165]\ttrain-mlogloss:0.25788\ttest-mlogloss:0.26845\n",
      "[166]\ttrain-mlogloss:0.25783\ttest-mlogloss:0.26846\n",
      "[167]\ttrain-mlogloss:0.25783\ttest-mlogloss:0.26846\n",
      "[168]\ttrain-mlogloss:0.25782\ttest-mlogloss:0.26846\n",
      "[169]\ttrain-mlogloss:0.25782\ttest-mlogloss:0.26846\n",
      "[170]\ttrain-mlogloss:0.25782\ttest-mlogloss:0.26846\n",
      "[171]\ttrain-mlogloss:0.25782\ttest-mlogloss:0.26846\n",
      "[172]\ttrain-mlogloss:0.25778\ttest-mlogloss:0.26845\n",
      "[173]\ttrain-mlogloss:0.25777\ttest-mlogloss:0.26846\n",
      "[174]\ttrain-mlogloss:0.25774\ttest-mlogloss:0.26846\n",
      "[175]\ttrain-mlogloss:0.25773\ttest-mlogloss:0.26846\n",
      "[176]\ttrain-mlogloss:0.25771\ttest-mlogloss:0.26847\n",
      "[177]\ttrain-mlogloss:0.25771\ttest-mlogloss:0.26846\n",
      "[178]\ttrain-mlogloss:0.25768\ttest-mlogloss:0.26847\n",
      "[179]\ttrain-mlogloss:0.25764\ttest-mlogloss:0.26849\n",
      "[180]\ttrain-mlogloss:0.25758\ttest-mlogloss:0.26852\n",
      "[181]\ttrain-mlogloss:0.25750\ttest-mlogloss:0.26853\n",
      "[182]\ttrain-mlogloss:0.25748\ttest-mlogloss:0.26855\n",
      "[183]\ttrain-mlogloss:0.25746\ttest-mlogloss:0.26856\n",
      "[184]\ttrain-mlogloss:0.25743\ttest-mlogloss:0.26856\n",
      "[185]\ttrain-mlogloss:0.25738\ttest-mlogloss:0.26856\n",
      "[186]\ttrain-mlogloss:0.25731\ttest-mlogloss:0.26859\n",
      "[187]\ttrain-mlogloss:0.25723\ttest-mlogloss:0.26860\n",
      "[188]\ttrain-mlogloss:0.25713\ttest-mlogloss:0.26861\n",
      "[189]\ttrain-mlogloss:0.25710\ttest-mlogloss:0.26861\n",
      "[190]\ttrain-mlogloss:0.25708\ttest-mlogloss:0.26863\n",
      "[191]\ttrain-mlogloss:0.25708\ttest-mlogloss:0.26863\n",
      "[192]\ttrain-mlogloss:0.25703\ttest-mlogloss:0.26863\n",
      "[193]\ttrain-mlogloss:0.25700\ttest-mlogloss:0.26863\n",
      "[194]\ttrain-mlogloss:0.25696\ttest-mlogloss:0.26863\n",
      "[195]\ttrain-mlogloss:0.25694\ttest-mlogloss:0.26862\n",
      "[196]\ttrain-mlogloss:0.25691\ttest-mlogloss:0.26864\n",
      "[197]\ttrain-mlogloss:0.25688\ttest-mlogloss:0.26863\n",
      "[198]\ttrain-mlogloss:0.25683\ttest-mlogloss:0.26862\n",
      "[199]\ttrain-mlogloss:0.25679\ttest-mlogloss:0.26865\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      1.00      0.93     44198\n",
      "         1.0       0.98      0.29      0.44      8882\n",
      "         2.0       1.00      1.00      1.00        15\n",
      "         3.0       1.00      1.00      1.00       480\n",
      "\n",
      "    accuracy                           0.88     53575\n",
      "   macro avg       0.96      0.82      0.84     53575\n",
      "weighted avg       0.89      0.88      0.85     53575\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "xg_train = xgb.DMatrix(X_train, label=y_train)\n",
    "xg_test = xgb.DMatrix( X_test, label=y_test)\n",
    "#1.训练模型\n",
    "# setup parameters for xgboost\n",
    "param = {}\n",
    "# use softmax multi-class classification\n",
    "param['objective'] = 'multi:softmax'\n",
    "# scale weight of positive examples\n",
    "param['eta'] = 0.2\n",
    "param['max_depth'] = 6\n",
    "param['silent'] = 1\n",
    "param['nthread'] = 4\n",
    "param['num_class'] = 4\n",
    "\n",
    "watchlist = [ (xg_train,'train'), (xg_test, 'test') ]\n",
    "num_round = 150\n",
    "bst = xgb.train(param, xg_train, num_round, watchlist );\n",
    "\n",
    "pred = bst.predict( xg_test );\n",
    "\n",
    "print(classification_report(y_test, pred)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-8a767fc116d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneural_network\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMLPClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLPClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lbfgs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_layer_sizes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/python_lyh/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    993\u001b[0m         \"\"\"\n\u001b[1;32m    994\u001b[0m         return self._fit(X, y, incremental=(self.warm_start and\n\u001b[0;32m--> 995\u001b[0;31m                                             hasattr(self, \"classes_\")))\n\u001b[0m\u001b[1;32m    996\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/python_lyh/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, incremental)\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolver\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'lbfgs'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m             self._fit_lbfgs(X, y, activations, deltas, coef_grads,\n\u001b[0;32m--> 375\u001b[0;31m                             intercept_grads, layer_units)\n\u001b[0m\u001b[1;32m    376\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/python_lyh/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_fit_lbfgs\u001b[0;34m(self, X, y, activations, deltas, coef_grads, intercept_grads, layer_units)\u001b[0m\n\u001b[1;32m    467\u001b[0m                     \u001b[0;34m\"gtol\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m                 },\n\u001b[0;32m--> 469\u001b[0;31m                 args=(X, y, activations, deltas, coef_grads, intercept_grads))\n\u001b[0m\u001b[1;32m    470\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_optimize_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lbfgs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/python_lyh/lib/python3.7/site-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'l-bfgs-b'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m         return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0;32m--> 610\u001b[0;31m                                 callback=callback, **options)\n\u001b[0m\u001b[1;32m    611\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tnc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         return _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n",
      "\u001b[0;32m~/.conda/envs/python_lyh/lib/python3.7/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, **unknown_options)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;31m# Overwrite f and g:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'NEW_X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0;31m# new iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/python_lyh/lib/python3.7/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36mfunc_and_grad\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m             \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/python_lyh/lib/python3.7/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[0;34m(*wrapper_args)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/python_lyh/lib/python3.7/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/python_lyh/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_loss_grad_lbfgs\u001b[0;34m(self, packed_coef_inter, X, y, activations, deltas, coef_grads, intercept_grads)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_coef_inter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         loss, coef_grads, intercept_grads = self._backprop(\n\u001b[0;32m--> 177\u001b[0;31m             X, y, activations, deltas, coef_grads, intercept_grads)\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_pack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoef_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintercept_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/python_lyh/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_backprop\u001b[0;34m(self, X, y, activations, deltas, coef_grads, intercept_grads)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;31m# Forward propagate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;31m# Get loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/python_lyh/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_forward_pass\u001b[0;34m(self, activations)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layers_\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             activations[i + 1] = safe_sparse_dot(activations[i],\n\u001b[0;32m--> 104\u001b[0;31m                                                  self.coefs_[i])\n\u001b[0m\u001b[1;32m    105\u001b[0m             \u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintercepts_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/python_lyh/lib/python3.7/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     if (sparse.issparse(a) and sparse.issparse(b)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-4, hidden_layer_sizes=(50,50), max_iter=200, random_state=1)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)  \n",
    "\n",
    "print(classification_report(y_test, y_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
